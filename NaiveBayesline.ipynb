{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaiveBayesline.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "vx7tVhvTrRLc"
      ],
      "authorship_tag": "ABX9TyOOuQdFd9hSe74GzjX8sWjI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beatobongco/naive_bayesline/blob/main/NaiveBayesline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rrXRBoA2SaI"
      },
      "source": [
        "# A great Naive Bayesline for classification \n",
        "\n",
        "Goal: understand the simple mechanics behind a really good classification baseline and implement it from scratch in Python to classify the sentiment of IMDB reviews.\n",
        "\n",
        "![img](https://i.redd.it/w34a8bl4cfc01.jpg)\n",
        "\n",
        "## Bayes' Theorem \n",
        "\n",
        "The probability of an event can be determined based on prior knowledge of conditions that might be related to the event.\n",
        "\n",
        "### Probabilities\n",
        "\n",
        "For a lot of things in NLP, we can think about the basis of our probabilities as \"counting\" things.\n",
        "\n",
        "A toy example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjIN9OxIjCiE",
        "outputId": "3546ccf5-4da0-436a-b812-0366a264ac46"
      },
      "source": [
        "# the model's tiny world\n",
        "tiny_corpus = \"hello hello hello world\"\n",
        "\n",
        "# count the num times hello appears / num total words\n",
        "# given the model's worldview, it thinks words it'll see conform to these probabilities\n",
        "hello_probability = 3 / 4 # 0.75\n",
        "world_probability = 1 / 4 # 0.25\n",
        "\n",
        "# Let's simulate \n",
        "import random\n",
        "\n",
        "n = 100000 # num simulations\n",
        "splitted = tiny_corpus.split()\n",
        "num_hello = 0\n",
        "num_world = 0\n",
        "\n",
        "for _ in range(n):\n",
        "  c = random.choice(splitted)\n",
        "  if c == \"hello\":\n",
        "    num_hello += 1\n",
        "  elif c == \"world\":\n",
        "    num_world += 1\n",
        "\n",
        "print(f\"Percentage \\\"hello\\\" was picked {num_hello / n * 100}%. \\\"hello\\\" probability derived from counts: {hello_probability}\")\n",
        "print(f\"Percentage \\\"world\\\" was picked {num_world / n * 100}%. \\\"world\\\" probability derived from counts: {world_probability}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage \"hello\" was picked 75.063%. \"hello\" probability derived from counts: 0.75\n",
            "Percentage \"world\" was picked 24.937%. \"world\" probability derived from counts: 0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwvLs1Tvm1T3"
      },
      "source": [
        "Naives Bayes is just like a souped up version of this!\n",
        "\n",
        "Given that the world for our model consists solely of documents (IMDB reviews) and labels (positive or negative), and documents consist of words, we can use the counts of the words and labels in each document to derive the probabilities.\n",
        "\n",
        "We can visualize it better if we think in terms of sets:\n",
        "\n",
        "Each box represents a review and we can see how many reviews are positive and contain the word \"happy\".\n",
        "\n",
        "We can represents this as a Venn diagram too!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksGX72zNz18w"
      },
      "source": [
        "![img](https://i.imgur.com/vMAaLwV.png)\n",
        "\n",
        "### Conditional probability\n",
        "What is the probability of a review being \"positive\" if it contains the word \"happy\"?\n",
        "\n",
        "Can be expressed as \n",
        "\n",
        "```\n",
        "P(positive | happy)\n",
        "```\n",
        "\n",
        "![img](https://imgur.com/45uAu4u.png)\n",
        "\n",
        "And we can get that by counting the following\n",
        "\n",
        "1. `P(positive) = no. positive reviews`\n",
        "2. `P(happy) = no. reviews with word \"happy\"`\n",
        "3. `P(positive ∩ happy) = no. positive reviews with the word happy`\n",
        "\n",
        "![img](https://i.imgur.com/KjVAaRC.png)\n",
        "\n",
        "We can think about evidence this way: given we have observed 4 tweets in our world containing the word \"happy\" (`P(happy)`), and of those tweets we know 3 of them are positive (`P(positive ∩ happy)`). \n",
        "\n",
        "If we trust that our evidence models real life somewhat accurately we can say\n",
        "\n",
        "```\n",
        "P(positive|happy) = P(positive ∩ happy) / P(happy)\n",
        "```\n",
        "\n",
        "For the flipped case\n",
        "\n",
        "```\n",
        "P(happy|positive) = P(happy ∩ positive) / P(positive)\n",
        "```\n",
        "\n",
        "From that we can derive Bayes Theorem:\n",
        "\n",
        "```\n",
        "# intersections are equivalent\n",
        "P(positive ∩ happy) == P(happy ∩ positive) \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hW8X0xcQgGH",
        "outputId": "26998259-9914-425f-e8e4-f067ec88e762"
      },
      "source": [
        "a = set([1, 2, 3])\n",
        "b = set([2, 3, 4])\n",
        "\n",
        "# throws an AssertionError if False!\n",
        "assert a.intersection(b) == b.intersection(a)\n",
        "\n",
        "print(f\"A ∩ B = {a.intersection(b)}\")\n",
        "print(f\"B ∩ A = {b.intersection(a)}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A ∩ B = {2, 3}\n",
            "B ∩ A = {2, 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crhHQ-hYQg7B"
      },
      "source": [
        "```\n",
        "# If\n",
        "P(happy|positive) = P(happy ∩ positive) / P(positive)\n",
        "\n",
        "# we can multiple both sides by P(positive)\n",
        "P(happy|positive) * P(positive) = P(happy ∩ positive)  \n",
        "\n",
        "# then substitute the intersection \n",
        "P(positive|happy) = P(happy|positive) * P(positive) / P(happy)\n",
        "```\n",
        "\n",
        "And that's Bayes Theorem!\n",
        "```\n",
        "P(A|B) = P(B|A) * P(A) / P(B)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ohmV12QB4HD"
      },
      "source": [
        "## Download dataset\n",
        "\n",
        "We can use huggingface's cool dataset library!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBAM1vY32DXW",
        "outputId": "efdbc742-0f7f-4cef-f9b5-e45e041ae803"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (1.1.3)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.4)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTJ_vgAj5RxS"
      },
      "source": [
        "from datasets import load_dataset\n",
        "from typing import List, Dict"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vI6r4fBI2JDf",
        "outputId": "26cbfe69-5a96-4253-ae49-31b9353b8aa2"
      },
      "source": [
        "dataset = load_dataset('imdb')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJxxzz0nFeib",
        "outputId": "e491e9df-59ee-442a-b1c7-f4ce95422fea"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    unsupervised: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njDu7HLNFUf0",
        "outputId": "bb86af11-fc8a-4e56-a570-e6d271a39124"
      },
      "source": [
        "dataset[\"train\"][0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 1,\n",
              " 'text': 'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQKF0ZvZFX4d",
        "outputId": "8fda6feb-a2e7-4c9b-b42e-9c57ecac80be"
      },
      "source": [
        "dataset[\"train\"][12500]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 0,\n",
              " 'text': \"Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\"}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ82yFPoC7Gy"
      },
      "source": [
        "## Preprocess dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdK-FDBT2Nu6"
      },
      "source": [
        "import re\n",
        "\n",
        "def preprocess(text: str) -> str:\n",
        "  # lowercase\n",
        "  t = text.lower()\n",
        "  # remove special characters\n",
        "  t = re.sub(\"[^\\w\\s]\", \" \", t)\n",
        "  t = re.sub(\"\\s\\s+\", \" \", t)\n",
        "  return t\n",
        "\n",
        "def preprocess_dataset(dataset) -> List[Dict]:\n",
        "  _dataset = []\n",
        "  for article in dataset:\n",
        "    _article = dict()\n",
        "    _article[\"label\"] = article[\"label\"]            \n",
        "    _article[\"text\"] = preprocess(article[\"text\"])\n",
        "    _dataset.append(_article)\n",
        "  return _dataset\n",
        "\n",
        "processed_data = preprocess_dataset(dataset[\"train\"])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "nggQryFUCrWg",
        "outputId": "173ccf2c-0124-40be-b7d5-3ac34f7dbcb8"
      },
      "source": [
        "preprocess(\"I am the eggman, they are the eggman...I am the walrus!Cuckoo\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'i am the eggman they are the eggman i am the walrus cuckoo'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKofoZzTB-z0"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "### The big ideas \n",
        "1. We can determine a word's class probability by its counts \n",
        "2. The class of a document can be determined by the class probability of each of its words \n",
        "3. We will assume each word doesn't affect other words (hence: Naive)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0B65OEdHhWB"
      },
      "source": [
        "### Python trivia: defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dbbs7tQ4F6fV"
      },
      "source": [
        "from collections import defaultdict"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKsX-DAhGGPa"
      },
      "source": [
        "# a defaultdict specifies a default when you are accessing keys that dont yet exist\n",
        "dd = defaultdict(int)\n",
        "dd[\"wowza\"]\n",
        "dd[\"coolio\"] += 1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kge9DaKYGK6n",
        "outputId": "1d1285dd-6068-4775-e6a4-e03b6fd15264"
      },
      "source": [
        "dd"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {'coolio': 1, 'wowza': 0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "3qsDajucHR3v",
        "outputId": "52fe210f-e253-43b2-8685-e2e9c295c099"
      },
      "source": [
        "dd2 = defaultdict(lambda: \"laugh out loud\")\n",
        "dd2[\"lol\"]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'laugh out loud'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ1GPwp8K2rg"
      },
      "source": [
        "### Ok, let's build forreals!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC1y-KRy2a5G"
      },
      "source": [
        "import math \n",
        "\n",
        "def build_model(dataset) -> Dict[str, Dict[str, float]]:\n",
        "  # create a dict where \n",
        "  # word -> {positive_class_count: int, negative_class_count: int, log_probability: float}\n",
        "  probs = defaultdict(lambda: defaultdict(int))\n",
        "  # count the num words of each class\n",
        "  totals = defaultdict(int)\n",
        "  for article in dataset:\n",
        "    # 0 for negative, 1 for positive\n",
        "    cls = article[\"label\"]    \n",
        "    for word in article[\"text\"].split():            \n",
        "      probs[word][cls] += 1\n",
        "      totals[cls] += 1\n",
        "\n",
        "  # calculate log probabilities\n",
        "  vocab_length = len(probs.keys())\n",
        "  for word in probs:\n",
        "    # laplace smoothing - to avoid dividing by 0\n",
        "    neg_prob = probs[word][0] + 1 / (totals[0] + vocab_length)\n",
        "    pos_prob = probs[word][1] + 1 / (totals[1] + vocab_length)\n",
        "\n",
        "    # why log? too bad you didn't listen to @alron boohoo\n",
        "    probs[word][\"logprob\"] = math.log(pos_prob / neg_prob)\n",
        "  \n",
        "  # not really needed for balanced classes\n",
        "  # count of pos articles / count of neg articles\n",
        "  prior = 1\n",
        "\n",
        "  return probs, prior\n",
        "\n",
        "model, prior = build_model(processed_data)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73XCUKgtOrHn",
        "outputId": "e8ee66ef-f875-4948-a51e-12dffdcf0936"
      },
      "source": [
        "model[\"not\"]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {0: 16359, 1: 14273, 'logprob': -0.13640856396196113})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cw1I2px6CluX",
        "outputId": "8ac6130b-9a76-401d-f8fd-2f10d8901ec3"
      },
      "source": [
        "model[\"good\"]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {0: 7423, 1: 7724, 'logprob': 0.03974907642828515})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F--KOudtHCxP",
        "outputId": "3e1d6bd6-ed6a-4d71-f85f-849bd203d57d"
      },
      "source": [
        "model[\"bad\"]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {0: 7401, 1: 1907, 'logprob': -1.3560837994737982})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF59URPUNlne",
        "outputId": "bfe11392-7b9a-47c3-c80c-e694e6cff571"
      },
      "source": [
        "print(prior)\n",
        "math.log(prior)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATY0M1djO6Bn"
      },
      "source": [
        "## Naive Bayes Inferencing \n",
        "\n",
        "We predict with the model with what's called the likelihood score\n",
        "\n",
        "![img](https://i.imgur.com/VpNd1Ng.png)\n",
        "\n",
        "### Log Likelihood\n",
        "\n",
        "To prevent numerical underflow as a result of multiplying many small number together\n",
        "\n",
        "Recall the **[First Law of ~Robotics~ Logarithms](https://www.mathcentre.ac.uk/resources/uploaded/mc-bus-loglaws-2009-1.pdf)**\n",
        "\n",
        "```\n",
        "log(A * B) = log(A) + log(B)\n",
        "```\n",
        "\n",
        "Let's kick numerical underflow's ass by using the **log** likelihood\n",
        "\n",
        "![img](https://i.imgur.com/mLupQyi.png)\n",
        "\n",
        "![img](https://i.imgur.com/pDTSv8N.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7wQRAsx8xrh"
      },
      "source": [
        "def predict(text, model, prior):  \n",
        "  prob = math.log(prior) # 0\n",
        "\n",
        "  # this part is called the \"log likelihood\"\n",
        "  for word in preprocess(text).split():\n",
        "    if word in model: # trivia: O(1) operation\n",
        "      prob += model[word][\"logprob\"] \n",
        "  return 1 if prob > 0 else 0"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX4ZyOrABLUz",
        "outputId": "f36fd180-daa6-4daa-c9e4-da5cee436649"
      },
      "source": [
        "predict(\"What a great movie! I cried tears of joy\", model, prior)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYznjqOXBS6n",
        "outputId": "ad14249b-d798-448c-c59d-983ed8485a8d"
      },
      "source": [
        "predict(\"What a shameful movie. I cant believe I wasted my money\", model, prior)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekXOyj1YPCGk"
      },
      "source": [
        "## Determine accuracy on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwXOUczX_fY5",
        "outputId": "255ab602-c90b-4483-bcaf-a9e8dfca87c5"
      },
      "source": [
        "mistakes = []\n",
        "def get_accuracy(model, prior):\n",
        "  global mistakes\n",
        "  correct = 0\n",
        "  for article in dataset[\"test\"]:\n",
        "    pred = predict(article[\"text\"], model, prior)\n",
        "    if pred == article[\"label\"]:\n",
        "      correct += 1\n",
        "    else:\n",
        "      mistakes.append(article)\n",
        "  print(f\"Accuracy is {correct / len(dataset['test']) * 100}%\")  \n",
        "\n",
        "get_accuracy(model, prior)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is 74.732%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TlYYA8O8to-"
      },
      "source": [
        "Let's examine our mistakes. \n",
        "\n",
        "Change the `seed` to get different results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrqUk9JE_kHf",
        "outputId": "d7db55ff-0e99-4843-f076-560dd9c759a9"
      },
      "source": [
        "i = 0\n",
        "random.seed(1337)\n",
        "random.shuffle(mistakes)\n",
        "for m in mistakes:  \n",
        "  t = m[\"text\"]\n",
        "  if len(t.split()) < 25:\n",
        "    i += 1\n",
        "    print(f\"{i}. Class: {m['label']} - {t}\")\n",
        "    print()\n",
        "    \n",
        "  if i == 10:\n",
        "    break"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. Class: 0 - More suspenseful, more subtle, much, much more disturbing....\n",
            "\n",
            "2. Class: 1 - This is a great movie. Too bad it is not available on home video.\n",
            "\n",
            "3. Class: 1 - For pure gothic vampire cheese nothing can compare to the Subspecies films. I highly recommend each and every one of them.\n",
            "\n",
            "4. Class: 0 - Widow hires a psychopath as a handyman. Sloppy film noir thriller which doesn't make much of its tension promising set-up. (3/10)\n",
            "\n",
            "5. Class: 1 - If you like Pauly Shore, you'll love Son in Law. If you hate Pauly Shore, then, well...I liked it!\n",
            "\n",
            "6. Class: 1 - As a big fan of Tiny Toon Adventures, I loved this movie!!! It was so funny!!! It really captured how cartoons spent their summers.\n",
            "\n",
            "7. Class: 1 - Very intelligent language usage of Ali, which you musn't miss! In one word: (eeh sentence...) Wicked, so keep it real and pass it on!\n",
            "\n",
            "8. Class: 1 - This is the greatest movie ever. If you have written it off with out ever seeing it. You must give it a second try.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbROojKlHRE7",
        "outputId": "40958a1b-cda4-433a-a5e9-f7316576070c"
      },
      "source": [
        "model[\"toon\"]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {0: 5, 1: 1, 'logprob': -1.6094376586208217})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5kOmAD8Q7Fs",
        "outputId": "e854186b-7c20-4824-ecbc-30157206b684"
      },
      "source": [
        "model[\"cheese\"]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {0: 119, 1: 39, 'logprob': -1.1155618415402542})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsSQIuExQ-W_",
        "outputId": "54858192-3e9b-4622-9194-39ef0e8d8503"
      },
      "source": [
        "model[\"suspenseful\"]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {0: 69, 1: 123, 'logprob': 0.5780778486492153})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgtLIDZEn9pP"
      },
      "source": [
        "## Improve accuracy with n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Goo5EACATtW-"
      },
      "source": [
        "def ngram(text, n):\n",
        "  word_list = text.split()\n",
        "  grams = []\n",
        "  for i in range(len(word_list) - n + 1):\n",
        "    grams.append(tuple(word_list[i:i+n]))\n",
        "  return grams\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Swwvl_lfUAbF",
        "outputId": "786e4876-ddeb-4819-b68b-81cb5d6ad967"
      },
      "source": [
        "# bigrams\n",
        "ngram(\"hello there that is a cute cat\", 2)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hello', 'there'),\n",
              " ('there', 'that'),\n",
              " ('that', 'is'),\n",
              " ('is', 'a'),\n",
              " ('a', 'cute'),\n",
              " ('cute', 'cat')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCdyClZaSUzt"
      },
      "source": [
        "def build_ngram_model(dataset) -> Dict[str, Dict[str, float]]:\n",
        "  \"\"\"Create a dictionary where the keys are the words in the vocabulary \n",
        "  \n",
        "    word -> {positive_class_count: int, \n",
        "             negative_class_count: int, \n",
        "             log_probability: float}\n",
        "  \"\"\"\n",
        "  probs = defaultdict(lambda: defaultdict(int))\n",
        "  # count the total words of each class\n",
        "  totals = defaultdict(int)\n",
        "  for article in dataset:\n",
        "    # 0 for negative, 1 for positive\n",
        "    cls = article[\"label\"]    \n",
        "    ngrams = ngram(article[\"text\"], 2) + ngram(article[\"text\"], 1)\n",
        "    for word in ngrams:            \n",
        "      probs[word][cls] += 1\n",
        "      totals[cls] += 1\n",
        "\n",
        "  # calculate log probabilities\n",
        "  vocab_length = len(probs.keys())\n",
        "  for word in probs:\n",
        "    # laplace smoothing - to avoid dividing by 0\n",
        "    neg_prob = probs[word][0] + 1 / (totals[0] + vocab_length)\n",
        "    pos_prob = probs[word][1] + 1 / (totals[1] + vocab_length)\n",
        "    probs[word][\"logprob\"] = math.log(pos_prob / neg_prob)\n",
        "  \n",
        "  # not really needed for balanced classes\n",
        "  prior = 1\n",
        "\n",
        "  return probs, prior\n",
        "\n",
        "ngram_model, ngram_prior = build_ngram_model(processed_data)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSiOw86vVMAA"
      },
      "source": [
        "def ngram_predict(text, model, prior):  \n",
        "  prob = math.log(prior) # 0\n",
        "\n",
        "  # this part is called the \"log likelihood\"\n",
        "  t = preprocess(text)\n",
        "  for word in ngram(t, 1) + ngram(t, 2):\n",
        "    if word in model: # trivia: O(1) operation\n",
        "      prob += model[word][\"logprob\"] \n",
        "  return 1 if prob > 0 else 0"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXmhRieCZSen",
        "outputId": "b335ac9b-fd73-4c75-d867-1f1460bf0165"
      },
      "source": [
        "ngram_predict(\"It was almost good but in the end it was not good\", ngram_model, ngram_prior)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeI25vcvZaGY",
        "outputId": "d6afe998-7a9d-4879-ab9e-f066537b262c"
      },
      "source": [
        "ngram_predict(\"Simply amazing. The director was sublime.\", ngram_model, ngram_prior)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9UlOAkVU7gj",
        "outputId": "0c8f8335-4a66-4646-dc9a-3d45a562d4da"
      },
      "source": [
        "def get_ngram_accuracy(model, prior):\n",
        "  correct = 0\n",
        "  mistakes = []\n",
        "  for article in dataset[\"test\"]:\n",
        "    pred = ngram_predict(article[\"text\"], model, prior)\n",
        "    if pred == article[\"label\"]:\n",
        "      correct += 1\n",
        "    else:\n",
        "      mistakes.append(article)\n",
        "  print(f\"Accuracy is {correct / len(dataset['test']) * 100}%\")  \n",
        "\n",
        "get_ngram_accuracy(ngram_model, ngram_prior)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is 80.184%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTvhnNrSoFnD"
      },
      "source": [
        "## NBSVM\n",
        "\n",
        "As described in the paper [Baselines and Bigrams: Simple, Good Sentiment and Topic Classification by Wang and Manning](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf)\n",
        "\n",
        "![img](https://i.imgflip.com/10av7r.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVo1l_7iU-fk"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
        "x = vectorizer.fit_transform([d[\"text\"] for d in processed_data])\n",
        "y = np.array([d[\"label\"] for d in processed_data])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpgcsU-FlNkJ",
        "outputId": "eed4d2be-3d56-4dc0-f9a8-04b9e817aa33"
      },
      "source": [
        "# each row is a bag of words representation of a document\n",
        "# (num_docs, vocab)\n",
        "x.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 1513832)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-aecUfjkTpw"
      },
      "source": [
        "def pr(x, y_i, y):\n",
        "  \"\"\"Count of each word belonging to class\n",
        "    divided by\n",
        "    How many docs in a class\n",
        "\n",
        "    count num happy / num positive documents\n",
        "  \"\"\"\n",
        "  # each word of y_i class's counts\n",
        "  # (1, vocab_size)\n",
        "  p = x[y==y_i].sum(0) \n",
        "\n",
        "  # counts of every positive document\n",
        "  # scalar \n",
        "  class_count = (y==y_i).sum()\n",
        "  # add 1 smoothing\n",
        "  return (p+1) / (class_count + 1) "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "KDM592mRkv5L",
        "outputId": "b024e528-71f5-4d50-ad66-dbbcd88b76a7"
      },
      "source": [
        "pos_probs = pr(x, 1, y)\n",
        "neg_probs = pr(x, 0, y)\n",
        "\n",
        "# each column represents a word's positive probability\n",
        "display(pos_probs)\n",
        "\n",
        "# each column represents a word's negative probability\n",
        "display(neg_probs)\n",
        "\n",
        "\n",
        "print(f\"Positive probabilities shape {pos_probs.shape}\")\n",
        "print(f\"Negative probabilities shape {neg_probs.shape}\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "matrix([[3.43972482e-03, 1.59987201e-04, 1.59987201e-04, ...,\n",
              "         1.59987201e-04, 7.99936005e-05, 7.99936005e-05]])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "matrix([[4.15966723e-03, 7.99936005e-05, 7.99936005e-05, ...,\n",
              "         7.99936005e-05, 1.59987201e-04, 1.59987201e-04]])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Positive probabilities shape (1, 1513832)\n",
            "Negative probabilities shape (1, 1513832)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDG2TdBukve0"
      },
      "source": [
        "# calculate log probability for each word\n",
        "r = np.log(pr(x,1,y) / pr(x,0,y))\n",
        "m = LogisticRegression()\n",
        "\n",
        "# apply the log probability as a weight to each of the words\n",
        "# multiplying this vector will apply each weight to each word of each review\n",
        "x_nb = x.multiply(r)\n",
        "\n",
        "# fit the model\n",
        "nbsvm = m.fit(x_nb, y)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSBSLb25pavB"
      },
      "source": [
        "test_x = [t[\"text\"] for t in preprocess_dataset(dataset[\"test\"])]\n",
        "test_x = vectorizer.transform(test_x)\n",
        "\n",
        "# dont forget to weight the words of the test set too!\n",
        "test_x = test_x.multiply(r)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbEtEz7pp6tY"
      },
      "source": [
        "preds = nbsvm.predict(test_x)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z76KhsBp_lL"
      },
      "source": [
        "test_y = np.array([t[\"label\"] for t in dataset[\"test\"]])"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8X1TRXTmAOx"
      },
      "source": [
        "### 90% accuracy!!\n",
        "\n",
        "It's pretty crazy that just by weighting the features and training a logistic regression model we can get to 90%! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C97yDSBcqAZ8",
        "outputId": "d8b59220-3f6f-4039-9336-95a339c4fa3b"
      },
      "source": [
        "print(f\"Accuracy: {(preds == test_y).sum() / test_y.shape[0] * 100}%\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 90.736%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0c6drwA9qJa"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "### Yay\n",
        "* Naive Bayes is a great baseline for classification!\n",
        "* fast to train: there's no gradient descent going on...just a lot of counting\n",
        "\n",
        "![img](https://i.imgur.com/3vkcfNz.png)\n",
        "\n",
        "* accuracy is pretty awesome! Using BERT nets us something like +2-3% only. Here's a [leaderboard](https://paperswithcode.com/sota/sentiment-analysis-on-imdb)\n",
        "\n",
        "### Nay\n",
        "* doesn't deal with word dependencies! \n",
        "* doesn't deal with word order! \n",
        "\n",
        "```\n",
        "I am not happy I'm dead == I am happy I'm not dead \n",
        "```\n",
        "\n",
        "Somewhat mitigated with n-grams...\n",
        "\n",
        "## Thank you!\n",
        "\n",
        "So...\n",
        "\n",
        "![img](https://i.redd.it/w34a8bl4cfc01.jpg)\n",
        "\n",
        "> Oftentimes it's team **Naive Boy**..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx7tVhvTrRLc"
      },
      "source": [
        "## Bonus: Jeremy Howard's modification\n",
        "\n",
        "Check out Jeremy Howard's notebook for a couple more perctage points! \n",
        "\n",
        "https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline\n",
        "\n",
        "Differences\n",
        "* Instead of CountVectorizer, use TFIDF weighted counts\n",
        "* TFIDF hyperparams: \n",
        "  * sublinear tf\n",
        "  * max_df\n",
        "  * min_df\n",
        "* Use higher C (less regularization) for LogisticRegression \n",
        "\n",
        "I *think* using these TFIDF features should generalize better"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD1OX_vqAkIc"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import re, string\n",
        "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
        "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
        "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
        "               smooth_idf=1, sublinear_tf=1 )\n",
        "train_x = vectorizer.fit_transform([d[\"text\"] for d in dataset[\"train\"]])\n",
        "train_y = np.array([d[\"label\"] for d in dataset[\"train\"]])\n",
        "\n",
        "test_x = [t[\"text\"] for t in preprocess_dataset(dataset[\"test\"])]\n",
        "test_x = vectorizer.transform(test_x)\n",
        "test_y = np.array([t[\"label\"] for t in dataset[\"test\"]])"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xs0w3OqYDcKa",
        "outputId": "d5f4a752-765b-4a59-df97-69d64a93f339"
      },
      "source": [
        "r = np.log(pr(train_x, 1, train_y) / pr(train_x, 0, train_y))\n",
        "\n",
        "m = LogisticRegression(C=4)\n",
        "\n",
        "x_nb = train_x.multiply(r)\n",
        "nbsvm = m.fit(x_nb, train_y)\n",
        "\n",
        "test_x = test_x.multiply(r)\n",
        "preds = nbsvm.predict(test_x)\n",
        "\n",
        "print(f\"Accuracy: {(preds == test_y).sum() / test_y.shape[0] * 100}%\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 90.048%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}